# -*- coding: utf-8 -*-
"""Admission Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TZz6CmbZFO7ANIoXXF9FyxREvSfPN3u8
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing essential libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('admission_predict (1).csv')

df.shape

df.head()

df.info()

"""# **Checking for null values**"""

df.isnull().sum()

"""No null values

---

# **Checking for duplicate values**
"""

def remove_duplicate(data):
    data.drop_duplicates(keep="first", inplace=True)
    return "Checked Duplicates"

# Removes Duplicates from train data
remove_duplicate(df)

"""No duplicates found

---




"""

df = df.rename(columns={'GRE Score': 'GRE', 'TOEFL Score': 'TOEFL', 'LOR ': 'LOR', 'Chance of Admit ': 'Probability'})
df.head()

# Removing the serial no. column
df.drop('Serial No.', axis='columns', inplace=True)

plt.figure(figsize=(15,5))
sns.boxplot(data = df);
plt.title("Box plot to understand the distribution of data from outliers perspective");

#Checking individual columns 

plt.figure(figsize=(15, 12))
plt.subplot(231)
sns.boxplot(y=df["GRE"], color='green');
plt.xlabel("GRE Score");
plt.ylabel("");
plt.title("GRE Score distribution");


plt.subplot(232)
sns.boxplot(y=df["TOEFL"], color='cyan');
plt.xlabel("TOFEL Score");
plt.ylabel("");
plt.title("TOEFL Score distribution");

plt.subplot(233)
sns.boxplot(data=df[[ "University Rating","SOP", "LOR"]]);
plt.title("SOP, LOR and University Rating distribution");

plt.subplot(234)
sns.boxplot(y = df["CGPA"], color='magenta');
plt.xlabel("CGPA");
plt.ylabel("");
plt.title("CGPA Distribution");

plt.subplot(235)
sns.boxplot(y=df["Research"]);
plt.xlabel("Research");
plt.ylabel("");
plt.title("Research distrbution");

plt.subplot(236)
sns.boxplot(y=df["Probability"], color='yellow');
plt.xlabel("Chance of Admit");
plt.ylabel("");
plt.title("Chance of Admit distrbution");

"""**Observation from Feature wise outlier detection :**
*   In total 2 outliers have been identified.
*   Features in which outlier has been identified are LOR and Chance of Admit.
*   Based on the observation the outliers are in the lower side of the value and can be a corner cases. In explorative analysis they can be viewed in depth to take a decision on how to handle this outliers.

---

# **Explorative Data Analysis :**

## **1.   GRE Score:**
"""

sns.displot(x=df["GRE"], kde=True, color='darkblue')
plt.title("GRE score distribution with density distribution");

"""**Observations for GRE Score :**

*   Score is distributed in the range of 290 to 340.
*   Peaks are observed between 310 to 330 indicating most applicants have scored in this range.
*   As high as 70 applicants have scored between 315 to 320.
*   The distribution looks normal and can be further confirmed by QQ-plot.

---









"""

#Comparison of GRE Score with Chance of Admit and Research :
sns.lmplot(y="Probability", x = "GRE", hue="Research", data=df);
plt.title("Effect of GRE Score on Chance of Admit with categorization of Research");

"""**Observations from Comaprision of GRE Score with Chance of Admit with Research :**

*   As the GRE Score increases So does the Chance of Admit.
*   Applicants who opts for research tends to have higher GRE Score and the Chance of Admit.

---

## **2.   TOEFL Score :**
"""

sns.displot(x=df["TOEFL"], kde=True, color='olive')
plt.title("TOFEL score distribution with density distribution");

"""**Observations from TOFEL Score :**


*   TOFEL scores are distributed in the range of 90 to 120.
*   As high as 80 to 85 applicants have scored between 105 to 107.
*   A good number of applicants have scored in between 97 to 113.
*   Distribution seems to be normal and can be confirmed by QQ-plot.

---








"""

#Comparison of TOFEL Score with Chance of Admit and Research :
sns.lmplot(y="Probability", x = "TOEFL", hue="Research", data=df);
plt.title("Effect of TOEFL Scoree on Chance of Admit with categorization of Research");

"""**Observations from TOFEL Score and Chance of Admit and Research:**
*   As the TOFEL Score increases So does the chance of admit.
*   Applicants who have opted for research tend to have higher chance of admit.
---

## **3.   University Ratings**
"""

sns.countplot(x=df["University Rating"]);
plt.title("Number of Universities for each star rating by applicants");

"""**Observations on University Rating :**

*   Close to 160 applicants have marked their university as 3 stars.

*   3 stars are followed by 2 stars for universities.
*   1 star is given by only 30 to 35 applicants which is lowest.






---



"""

#Comparison of University Rating with Chance of Admit and Research :
sns.lmplot(y="Probability", x = "University Rating", hue="Research", data=df);
plt.title("Effect of University Rating on Chance of Admit with categorization of Research");

"""**Observation of University Rating with Chance of Admit and Research :**
*   As University Rating increases so does the chance of admit.
*   Universities with higher rating have more research opted applicants.
---

## **4. SOP: Statement of Purpose**
"""

sns.displot(x=df["SOP"], kde=True, color='green');
plt.title("Distribution for rattings on SOP");

"""**Observations from statement of purpose :**

*   Most of the applicants have received 3.5 to 4.0 stars in their SOP.

*   Based on the plot, rating distribution can be bucketed in 3 sections 1.0 to 2.1 as low ratings, 2.5 to 3.6 as mid level ratings and 3.9 to 5.0 as high level of ratings.
*   Each section described in the above point can further be divided into low, mid and high.This provides a really well quantified information on strength of statement of purpose.


---






 
"""

#Comparison of Statement of Purpose (SOP) with Chance of Admit and Research :
sns.lmplot(y="Probability", x = "SOP", hue="Research", data=df);
plt.title("Effect of SOP on Chance of Admit with categorization of Research");

"""**Observations from SOP with Chance of Admit and Research :**

*   Strength of Statement of purpose has a great impact on Chance of Admit.
*   Applicant with higher score on SOP and opted for research have almost sure shot at chance of admit.

---

## **5.   LOR : Letter of Recommendations**
"""

sns.displot(x=df["LOR"], kde=True, color='red');
plt.title("Distribution of rattings given to Letter of recommendation of applicants");

sns.scatterplot(x=df.index, y = df["LOR"], hue=df["LOR"]);
plt.title("Scatter visualization to identify the presence of outlier observed on boxplot");

"""**Observations for LOR :**

*   Most applicants have a recommendation ratting of 3.0.
*   A very good number of applicants are having LOR ratting between 3 to 4 inclusive.
*   The outlier observed in boxplot is basically just one applicant having the lowest rating of 1.

---


"""

#Comparision of Letter of Recommendations (LOR) with Chance of Admit and Research :
sns.lmplot(y="Probability", x = "LOR", hue="Research", data=df);
plt.title("Effect of LOR on Chance of Admit with categorization of Research");

"""**Observations from LOR score with Chance of Admit and Research :**

*   As the score of LOR improves , the Chance of Admit also improves with it.
*   From the graph if the applicant has opted to research then chances of admit is higher.
---

## **6.   CGPA**
"""

sns.displot(x=df["CGPA"], kde=True, color='darkred');
plt.title("Distribution of CGPA Score of applicants");

"""**Observations for CGPA :**

*   CGPA scores are distributed in range of 6.9 to 9.7 inclusive.
*   More than 70 applicants have the CGPA of 8.56.
*   Saturation can be observed as a great number of applicants have a CGPA between 7.90 to 9.25.

---


"""

#Comparison of CGPA with Chance of Admit and Research :
sns.lmplot(y="Probability", x = "CGPA", hue="Research", data=df);
plt.title("Effect of CGPA on Chance of Admit with categorization of Research");

"""**Observations from CGPA with Chance of Admit and Research :**

*   As the CGPA increases the chance of Admit increases tremendously.
*   Option of research with CGPA donot seems to have much of the impact compare to other features.
---

## **7.   Research Paper**
"""

sns.countplot(x=df["Research"]);
plt.title("Distribution of applicants who opted for research");

"""**Observations on Research :**

*   Out of 500 applicants approx 280 applicants have opted for research.
*   In precentage 56% applicants are chosing for research option.

---


"""

#Comparison of Research Opted with Chance of Admit :
sns.lmplot(y="Probability", x = "Research", hue='Research', data=df);
plt.title("Effect of Reasearch on Chance of Admit ");

"""**Observations from Research with Chance of Admit :**

*   Opting for research doesn't seem to have much effect on their chances of admission.

---

## **8.   Chance of Admission**
"""

sns.displot(x=df["Probability"], kde=True, color='brown');
plt.title("Distribution of Chance of Admit ");

plt.figure(figsize=(15,7))
sns.scatterplot(x=df.index, y = df["Probability"], hue=df["Probability"]);
plt.title("Scatter visualization to identify the presence of outlier observed on boxplot");

"""**Observations from Chance of admit :**

*   Chances are distributed between 0.34 to 0.97 inclusive.
*   Highest number of applicants have chance of admit at 0.72.
*   Most number of students have a chance of admmit above 0.61.
*   The one outlier that was observed in box plot was because of one student having a low chance of 0.34 which is also the minimum value registerd in the data.

---


"""

plt.figure(figsize=(10,10))
sns.heatmap(df.corr(), annot=True, cmap="YlGnBu");
plt.title("Correlation of features");

"""**Observations from multivariate analysis :**

*   CGPA Score has the most impact on Chance of Admit.
*   It is followed by GRE and TOFEL Score.
*   Research option has the least impact on the chance of admit.
*   University Ranking, statement of purpose and the letter of recommendation are also having impact on the chance of admit.
---

# **Feature Engineering and Modelling :**
"""

df.shape

df= df.reset_index(drop=True)

df.head()

df_copy = df.copy(deep=True)

# Splitting the dataset in features and label
X = df_copy.drop('Probability', axis='columns')
y = df_copy['Probability']

X.head()

y.head()

y=y.to_frame()

from sklearn.preprocessing import MinMaxScaler
sc_X = MinMaxScaler()
sc_y = MinMaxScaler()
X = sc_X.fit_transform(X)
y = sc_y.fit_transform(y)

X

y

# Splitting the dataset into train and test samples
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)
print(len(X_train), len(X_test))

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor

from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

Regressors = {'Linear Regression' : LinearRegression(normalize=True),
                            
               'Lasso' : Lasso(normalize=True),
               
               'SVR' : SVR(gamma='auto'),
              
               'Decision Tree' : DecisionTreeRegressor(criterion='mse', splitter='random'),
              
               'Random Forest' : RandomForestRegressor(n_estimators=20),

               'KNeighbors' : KNeighborsRegressor(n_neighbors=20)
 }

print( list(Regressors.keys()) )
print( list(Regressors.values()) )

tab=pd.DataFrame()

def mod(name,model,x,y):
  model.fit(x, y)

  scores = cross_val_score(model, x, y, cv=5)
  v=round(sum(scores)*100/len(scores),3)

  mae=round(mean_absolute_error(y_test,model.predict(X_test)),3)

  mse=round(mean_squared_error(y_test,model.predict(X_test)),3)

  rmse=round(np.sqrt(mean_squared_error(y_test,model.predict(X_test))),3)

  r2 = round(r2_score(y_test,model.predict(X_test)),3)

  n=40
  k=2
  adj_r2 = round(1 - ((1-r2)*(n-1)/(n-k-1)),3)

  result=[name ,v, mae, mse, rmse, r2, adj_r2]
  return (result)

for i in range(0,6):
  tab=pd.concat([tab,pd.DataFrame(mod(list(Regressors.keys())[i], list(Regressors.values())[i], X_train, y_train)).transpose()], axis=0, ignore_index=True)

tab.set_axis(['Model', 'Cross Val Score', 'MAE','MSE','RMSE','R2 Score','Adj R2 Score'], axis='columns', inplace=True)

tab

"""Since the **Linear Regression algorithm has the highest accuracy**, the model selected for this problem is Linear Regression."""

# Creating Linear Regression Model
lin_reg=LinearRegression(normalize=True)
lin_reg.fit(X_train, y_train)
lin_reg.score(X_test, y_test)

round(
    sc_y.inverse_transform(
        lin_reg.predict(sc_X.transform([[320, 113, 2, 2.0, 2.5, 8.64, 1]])))[0][0]*100,2)

"""A student has following details:
*   GRE Score: 320
*   TOEFL Score: 113
*   University Rating: 2
*   SOP Rating: 2
*   LOR Rating: 2.5
*   CGPA: 8.64
*   Opted for Research: Yes
*   **Chances of Admission**: 73.63%

---




"""

round(
    sc_y.inverse_transform(
        lin_reg.predict(sc_X.transform([[337, 118, 4, 4.5, 4.5, 9.65, 0]])))[0][0]*100,2)

"""A student has following details:
*   GRE Score: 337
*   TOEFL Score: 118
*   University Rating: 4
*   SOP Rating: 4.5
*   LOR Rating: 4.5
*   CGPA: 9.65
*   Opted for Research: No
*   **Chances of Admission**: 92.85%

---




"""